This is the README file for sprint-1

The dataset given by the IBM platform was not very efficient since the model overfitted the training data quickly not leading to greater accuracy. So in an effort to increase the efficiency of the classification algorithm, we have taken an initiative to collect the dataset overselves. We have used Python Selenium library to automate the dataset collection using webscraping and we have attached the required code in this sprint.

# Tasks done:
1. A front-end code (in HTML) which accepts user-inputted image that needs to be classified and its corresponding CSS file - home.html
2. A Machine Learning code that classifies the inputted image(works on the dataset provided by IBM) - Model_OriginalData_With_Augmentation.ipynb
3. A Machine Learning code that classifies the inputted image(works on the dataset collected by us) - Model_Without_Augmentation_New_Data.ipynb
4. Dataset Webscrapper code - webscrapper.py
5. A python file which acts as the backend script(server-side) and helps in redirection to various subdomains in the application - main.py
6. A front-end code (in HTML) which provides a clean and consistent layout for every HTML file to adopt, helps in a neat presentation - layout.html
7. A front-end code (in HTML) which shows the image uploaded by the user along with the prediction given by the algorithm - showimage.html

Dataset link - 
