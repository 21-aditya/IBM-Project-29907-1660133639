This is the README file for sprint-1

The dataset given by the IBM platform was not very efficient since the model overfitted the training data quickly not leading to greater accuracy. So in an effort to increase the efficiency of the classification algorithm, we have taken an initiative to collect the dataset overselves. We have used Python Selenium library to automate the dataset collection using webscraping and we have attached the required code in this sprint.

# Tasks done:
1. A front-end code (in HTML) which accepts user-inputted image that needs to be classified and its corresponding CSS file - submitted as a .html file.
2. A Machine Learning code that classifies the inputted image(works on the dataset provided by IBM) - submitted as a .pynb file.
3. A Machine Learning code that classifies the inputted image(works on the dataset collected by us) - submitted as a .pynb file.
4. Dataset Webscrapper code - submitted as .py file.
5. A python file which acts as the backend script(server-side) and helps in redirection to various subdomains in the application - main.py.

Dataset link - 
